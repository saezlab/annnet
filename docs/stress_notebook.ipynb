{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1042de8d",
   "metadata": {},
   "source": [
    "# Graph — End‑to‑End Stress Notebook\n",
    "\n",
    "**Goal:** build a realistic, multi‑slice biological interaction graph with **tens of thousands** of vertices and a mix of **binary edges, hyperedges, and vertex–edge (edge‑entity) links**, then **exercise every public API**: slices, presence queries, propagation (`shared` / `all`), views, analytics, set operations, aggregations, subgraph/copy, deletions, auditing, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0184fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust import of Graph\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from time import perf_counter\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))  # must be the parent folder that CONTAINS 'annnet'\n",
    "\n",
    "from annnet.core.graph import Graph\n",
    "\n",
    "G = Graph(directed=True)\n",
    "\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab386c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters — choose a scale\n",
    "# - DEMO runs fast on laptops\n",
    "# - STRESS creates 10^4–10^5 scale objects; adjust upward to your machine limits\n",
    "\n",
    "SCALE = \"STRESS\"  # \"DEMO\" or \"STRESS\"\n",
    "\n",
    "if SCALE.upper() == \"DEMO\":\n",
    "    N_PROTEINS = 5_00\n",
    "    N_TRANSCRIPTS = 2_00\n",
    "    N_METABOLITES = 1_00\n",
    "    N_EDGE_ENTITIES = 40\n",
    "    N_BIN_EDGES = 25_00  # binary protein-protein interactions (base slice)\n",
    "    N_HYPER_COMPLEX = 1_00  # undirected complexes\n",
    "    N_HYPER_CASCADE = 1_00  # directed signaling cascades\n",
    "    N_vertex_EDGE_BIDIR = 2_00  # vertex<->edge-entity links (counted as pairs)\n",
    "elif SCALE.upper() == \"MID\":\n",
    "    N_PROTEINS = 10_000\n",
    "    N_TRANSCRIPTS = 5_000\n",
    "    N_METABOLITES = 20_000\n",
    "    N_EDGE_ENTITIES = 3_500\n",
    "    N_BIN_EDGES = 45_000\n",
    "    N_HYPER_COMPLEX = 4, 000  # use commas? We'll correct below to int\n",
    "    N_HYPER_CASCADE = 3_000\n",
    "    N_vertex_EDGE_BIDIR = 5_000\n",
    "else:\n",
    "    N_PROTEINS = 200_000\n",
    "    N_TRANSCRIPTS = 50_000\n",
    "    N_METABOLITES = 20_000\n",
    "    N_EDGE_ENTITIES = 20_500\n",
    "    N_BIN_EDGES = 800_000\n",
    "    N_HYPER_COMPLEX = 20, 000  # use commas? We'll correct below to int\n",
    "    N_HYPER_CASCADE = 20_000\n",
    "    N_vertex_EDGE_BIDIR = 15_000\n",
    "\n",
    "# fix typo for N_HYPER_COMPLEX in STRESS case\n",
    "if isinstance(N_HYPER_COMPLEX, tuple):\n",
    "    N_HYPER_COMPLEX = 20000\n",
    "\n",
    "sliceS = [\"Healthy\", \"Stressed\", \"Disease\", \"DrugA\", \"DrugB\"]\n",
    "ORDERED_FOR_TEMPORAL = [\"Healthy\", \"Stressed\", \"Disease\", \"DrugA\", \"DrugB\"]\n",
    "\n",
    "# How many parallel edges to create as duplicates between random pairs\n",
    "N_PARALLEL_DUPES = max(1, N_BIN_EDGES // 20)\n",
    "\n",
    "# Fraction of vertices seeded into each non-default slice (to make propagate='shared'/'all' meaningful)\n",
    "SEED_FRAC_PER_slice = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8886718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "\n",
    "def rand_weight(base=1.0, jitter=0.5):\n",
    "    # positive weight with variability\n",
    "    w = base + (random.random() - 0.5) * 2 * jitter\n",
    "    return max(0.01, w)\n",
    "\n",
    "\n",
    "def try_to_pandas(df):\n",
    "    if df is None:\n",
    "        return None\n",
    "    if \"polars\" in type(df).__module__.lower():\n",
    "        return df.to_pandas() if hasattr(df, \"to_pandas\") else None\n",
    "    return df  # assume already pandas-like\n",
    "\n",
    "\n",
    "def head_df(df, n=5):\n",
    "    p = try_to_pandas(df)\n",
    "    return p.head(n) if p is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1f5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slices ready: ['Healthy', 'Stressed', 'Disease', 'DrugA', 'DrugB'] active: Healthy\n"
     ]
    }
   ],
   "source": [
    "# Build graph & slices\n",
    "t0 = perf_counter()\n",
    "G = Graph(directed=True)\n",
    "\n",
    "for lid in sliceS:\n",
    "    G.add_slice(lid, desc=f\"condition={lid}\")\n",
    "G.set_active_slice(sliceS[0])\n",
    "build_slices_time = perf_counter() - t0\n",
    "print(\"slices ready:\", G.list_slices(), \"active:\", G.get_active_slice())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c91253-6eb4-4273-a1ee-a3991284af9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l1boll/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---- progress helpers ----\n",
    "from time import perf_counter\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm  # uses notebook bar if available\n",
    "\n",
    "    _TQDM = True\n",
    "except Exception:\n",
    "    _TQDM = False\n",
    "\n",
    "\n",
    "def prog_iter(it, total=None, desc=\"\", mininterval=0.25):\n",
    "    \"\"\"Wrap any iterable with a progress display (tqdm if available, else no-op).\"\"\"\n",
    "    if _TQDM:\n",
    "        return tqdm(it, total=total, desc=desc, mininterval=mininterval, leave=False)\n",
    "    return it\n",
    "\n",
    "\n",
    "def batched(iterable, batch_size):\n",
    "    \"\"\"Yield lists of size <= batch_size (Py<3.12 compatible).\"\"\"\n",
    "    buf = []\n",
    "    for x in iterable:\n",
    "        buf.append(x)\n",
    "        if len(buf) == batch_size:\n",
    "            yield buf\n",
    "            buf = []\n",
    "    if buf:\n",
    "        yield buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "t = perf_counter()\n",
    "\n",
    "proteins = [f\"P{i}\" for i in range(1, N_PROTEINS + 1)]\n",
    "transcripts = [f\"T{i}\" for i in range(1, N_TRANSCRIPTS + 1)]\n",
    "metabolites = [f\"M{i}\" for i in range(1, N_METABOLITES + 1)]\n",
    "edge_entities = [f\"EE{i}\" for i in range(1, N_EDGE_ENTITIES + 1)]\n",
    "\n",
    "# --- Seed vertices in \"Healthy\" ---\n",
    "kinase_mask = rng.random(len(proteins)) < 0.15\n",
    "G.add_vertices_bulk(\n",
    "    (\n",
    "        {\"vertex_id\": p, \"kind\": \"protein\", **({\"family\": \"kinase\"} if km else {})}\n",
    "        for p, km in zip(proteins, kinase_mask)\n",
    "    ),\n",
    "    slice=\"Healthy\",\n",
    ")\n",
    "G.add_vertices_bulk(({\"vertex_id\": t, \"kind\": \"transcript\"} for t in transcripts), slice=\"Healthy\")\n",
    "G.add_vertices_bulk(({\"vertex_id\": m, \"kind\": \"metabolite\"} for m in metabolites), slice=\"Healthy\")\n",
    "\n",
    "# --- Edge-entities in \"Healthy\" (bulk) ---\n",
    "pathways = np.array([\"glycolysis\", \"tca\", \"mapk\", \"pi3k\"])\n",
    "drawn_pathways = pathways[rng.integers(0, len(pathways), size=len(edge_entities))]\n",
    "G.add_edge_entities_bulk(\n",
    "    (\n",
    "        {\"edge_entity_id\": ee, \"role\": \"enzyme\", \"pathway\": pw}\n",
    "        for ee, pw in zip(edge_entities, drawn_pathways)\n",
    "    ),\n",
    "    slice=\"Healthy\",\n",
    ")\n",
    "\n",
    "# --- Seed presence into other slices (bulk per slice) ---\n",
    "p_keep = SEED_FRAC_PER_slice\n",
    "for lid in sliceS[1:]:\n",
    "    pmask = rng.random(len(proteins)) < p_keep\n",
    "    tmask = rng.random(len(transcripts)) < p_keep\n",
    "    mmask = rng.random(len(metabolites)) < p_keep\n",
    "\n",
    "    G.add_vertices_bulk(\n",
    "        ({\"vertex_id\": p, \"kind\": \"protein\"} for p, keep in zip(proteins, pmask) if keep),\n",
    "        slice=lid,\n",
    "    )\n",
    "    G.add_vertices_bulk(\n",
    "        ({\"vertex_id\": t, \"kind\": \"transcript\"} for t, keep in zip(transcripts, tmask) if keep),\n",
    "        slice=lid,\n",
    "    )\n",
    "    G.add_vertices_bulk(\n",
    "        ({\"vertex_id\": m, \"kind\": \"metabolite\"} for m, keep in zip(metabolites, mmask) if keep),\n",
    "        slice=lid,\n",
    "    )\n",
    "\n",
    "build_vertices_time = perf_counter() - t\n",
    "print(\n",
    "    \"vertices done. #vertices:\",\n",
    "    G.number_of_vertices(),\n",
    "    \"Edge-entities:\",\n",
    "    sum(1 for k, v in G.entity_types.items() if v == \"edge\"),\n",
    "    \"time(s)=\",\n",
    "    round(build_vertices_time, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b171451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary edges (PPIs mostly among proteins), defined in Healthy then sliceed variants\n",
    "from time import perf_counter\n",
    "\n",
    "t = perf_counter()\n",
    "\n",
    "# ---------- 1) Bulk create binary edges on \"Healthy\" ----------\n",
    "pairs = []\n",
    "need = N_BIN_EDGES\n",
    "names = proteins\n",
    "n = len(names)\n",
    "\n",
    "# Generate candidate pairs quickly; reject self-loops\n",
    "while len(pairs) < need:\n",
    "    k = min(need - len(pairs), max(1024, need // 4))\n",
    "    us = random.choices(names, k=k)\n",
    "    vs = random.choices(names, k=k)\n",
    "    for u, v in zip(us, vs):\n",
    "        if u != v:\n",
    "            pairs.append((u, v))\n",
    "        if len(pairs) == need:\n",
    "            break\n",
    "\n",
    "dirs = [random.random() < 0.8 for _ in range(need)]\n",
    "ws = [rand_weight(1.2, 0.6) for _ in range(need)]\n",
    "\n",
    "bulk = [\n",
    "    {\n",
    "        \"source\": u,\n",
    "        \"target\": v,\n",
    "        \"weight\": w,\n",
    "        \"edge_directed\": d,\n",
    "        \"edge_type\": \"regular\",\n",
    "        \"slice\": \"Healthy\",\n",
    "    }\n",
    "    for (u, v), w, d in zip(pairs, ws, dirs)\n",
    "]\n",
    "ppis = G.add_edges_bulk(bulk, slice=\"Healthy\")  # list of edge_ids\n",
    "\n",
    "# ---------- 2) Bulk add parallel dupes ----------\n",
    "if ppis and N_PARALLEL_DUPES > 0:\n",
    "    chosen = random.choices(ppis, k=N_PARALLEL_DUPES)\n",
    "    par_edges = []\n",
    "    for eid in chosen:\n",
    "        u, v, _ = G.edge_definitions[eid]\n",
    "        par_edges.append(\n",
    "            {\n",
    "                \"source\": u,\n",
    "                \"target\": v,\n",
    "                \"weight\": rand_weight(1.0, 0.3),\n",
    "                \"edge_type\": \"regular\",\n",
    "                \"slice\": \"Healthy\",\n",
    "            }\n",
    "        )\n",
    "    G.add_edges_bulk(par_edges, slice=\"Healthy\")\n",
    "\n",
    "# ---------- 3) Bulk per-slice variants ----------\n",
    "base_w = {eid: G.edge_weights[eid] for eid in ppis}\n",
    "\n",
    "for lid in sliceS[1:]:\n",
    "    # Add all PPI edges to this slice in one shot\n",
    "    G.add_edges_to_slice_bulk(lid, ppis)\n",
    "\n",
    "    # Compute modifiers and upsert all weights for this slice at once\n",
    "    weights_rows = []\n",
    "    for eid in ppis:\n",
    "        bw = base_w[eid]\n",
    "        factor = {\n",
    "            \"Stressed\": rand_weight(1.10, 0.10),\n",
    "            \"Disease\": (0.7 if random.random() < 0.4 else rand_weight(1.30, 0.15)),\n",
    "            \"DrugA\": rand_weight(0.9, 0.25),\n",
    "            \"DrugB\": rand_weight(1.2, 0.20),\n",
    "        }[lid]\n",
    "        weights_rows.append((eid, {\"weight\": bw * factor, \"note\": f\"slice={lid}\"}))\n",
    "\n",
    "    G.set_edge_slice_attrs_bulk(lid, weights_rows)\n",
    "\n",
    "build_binary_time = perf_counter() - t\n",
    "print(\n",
    "    \"Binary edges built:\",\n",
    "    len(ppis),\n",
    "    \"total edges now:\",\n",
    "    G.number_of_edges(),\n",
    "    \"time(s)=\",\n",
    "    round(build_binary_time, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagation semantics via add_edge(..., propagate=...)\n",
    "t = perf_counter()\n",
    "# Ensure varied vertex presence across slices for a few pairs\n",
    "pairs = [(random.choice(proteins), random.choice(transcripts)) for _ in range(2000)]\n",
    "for u, v in pairs:\n",
    "    # 'shared': only slices where both endpoints already present\n",
    "    G.add_edge(\n",
    "        u, v, slice=\"Healthy\", edge_type=\"regular\", weight=rand_weight(0.8, 0.2), propagate=\"shared\"\n",
    "    )\n",
    "pairs2 = [(random.choice(proteins), random.choice(metabolites)) for _ in range(2000)]\n",
    "for u, v in pairs2:\n",
    "    # 'all': appears everywhere either endpoint exists (pulls other endpoint in)\n",
    "    G.add_edge(\n",
    "        u, v, slice=\"Healthy\", edge_type=\"regular\", weight=rand_weight(0.8, 0.2), propagate=\"all\"\n",
    "    )\n",
    "\n",
    "build_propagation_time = perf_counter() - t\n",
    "print(\"Propagation examples added (shared/all).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d79ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperedges: undirected complexes, directed cascades\n",
    "t = perf_counter()\n",
    "\n",
    "complex_ids = []\n",
    "for _ in range(N_HYPER_COMPLEX):\n",
    "    size = random.choice([3, 4, 5, 6])\n",
    "    members = set(random.sample(proteins, size))\n",
    "    hid = G.add_hyperedge(\n",
    "        members=members, slice=\"Healthy\", weight=rand_weight(1.0, 0.2), tag=\"complex\"\n",
    "    )\n",
    "    complex_ids.append(hid)\n",
    "    for lid in sliceS[1:]:\n",
    "        G.add_edge_to_slice(lid, hid)\n",
    "\n",
    "cascade_ids = []\n",
    "tries = 0\n",
    "while len(cascade_ids) < N_HYPER_CASCADE and tries < N_HYPER_CASCADE * 5:\n",
    "    tries += 1\n",
    "    head = set(random.sample(proteins, random.choice([1, 2])))\n",
    "    tail = set(random.sample(proteins, random.choice([2, 3, 4])))\n",
    "    if head & tail:\n",
    "        continue\n",
    "    hid = G.add_hyperedge(\n",
    "        head=head, tail=tail, slice=\"Healthy\", weight=rand_weight(1.0, 0.4), tag=\"cascade\"\n",
    "    )\n",
    "    cascade_ids.append(hid)\n",
    "    for lid in sliceS[1:]:\n",
    "        G.add_edge_to_slice(lid, hid)\n",
    "\n",
    "build_hyper_time = perf_counter() - t\n",
    "print(\"Hyperedges built: complexes=\", len(complex_ids), \"cascades=\", len(cascade_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7e391-094e-4712-97f3-8d10205de236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building hybrid vertex–edge reactions...\")\n",
    "\n",
    "t = perf_counter()\n",
    "\n",
    "for _ in range(N_vertex_EDGE_BIDIR):\n",
    "    ee = random.choice(edge_entities)\n",
    "    s  = random.choice(proteins + transcripts + metabolites)\n",
    "    tvertex = random.choice(proteins + transcripts + metabolites)\n",
    "\n",
    "    # Ensure entity exists in the slice (cheap if already added)\n",
    "    G.add_edge_entity(ee, slice=\"Healthy\")\n",
    "\n",
    "    # Vertex → Edge-Entity\n",
    "    eid1 = G.add_edge(\n",
    "        s, ee,\n",
    "        slice=\"Healthy\",\n",
    "        weight=rand_weight(1.0, 0.5)\n",
    "    )\n",
    "\n",
    "    # Edge-Entity → Vertex\n",
    "    eid2 = G.add_edge(\n",
    "        ee, tvertex,\n",
    "        slice=\"Healthy\",\n",
    "        weight=rand_weight(1.0, 0.5)\n",
    "    )\n",
    "\n",
    "    # Reflect edges to all other slices\n",
    "    for lid in sliceS[1:]:\n",
    "        G.add_edge_to_slice(lid, eid1)\n",
    "        G.add_edge_to_slice(lid, eid2)\n",
    "\n",
    "build_time = perf_counter() - t\n",
    "\n",
    "# ----------------------------\n",
    "# Results\n",
    "# ----------------------------\n",
    "print(f\"Hybrid vertex–edge reactions added: {N_vertex_EDGE_BIDIR}\")\n",
    "print(f\"Total time: {build_time:.4f} seconds\")\n",
    "print(f\"Average per reaction: {build_time / N_vertex_EDGE_BIDIR:.6f} seconds\")\n",
    "print(f\"Graph entities: {len(G.entity_to_idx)}\")\n",
    "print(f\"Graph edges:    {len(G.edge_to_idx)}\")\n",
    "\n",
    "build_vertexedge_time = perf_counter() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b56ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity & counts\n",
    "print(\"vertices:\", G.number_of_vertices(), \"Edges:\", G.number_of_edges())\n",
    "assert G.number_of_vertices() > 0 and G.number_of_edges() > 0\n",
    "\n",
    "# Edge-entity count\n",
    "edge_entity_count = sum(\n",
    "    1 for _id, et in G.entity_types.items() if et == \"edge\" and _id in set(edge_entities)\n",
    ")\n",
    "print(\"Edge-entities:\", edge_entity_count)\n",
    "assert edge_entity_count == len(edge_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Views & Top edges per slice\n",
    "try:\n",
    "    for lid in sliceS:\n",
    "        EV = G.edges_view(slice=lid, resolved_weight=True)\n",
    "        print(\n",
    "            f\"[{lid}] edges_view rows:\", getattr(EV, \"height\", getattr(EV, \"shape\", [\"?\", \"?\"])[0])\n",
    "        )\n",
    "        # filter binary only, sort by effective weight\n",
    "        if pl is not None and isinstance(EV, pl.DataFrame):\n",
    "            top = (\n",
    "                EV.filter(pl.col(\"kind\") == \"binary\")\n",
    "                .sort(\"effective_weight\", descending=True)\n",
    "                .select([\"edge_id\", \"source\", \"target\", \"effective_weight\"])\n",
    "                .head(5)\n",
    "            )\n",
    "            print(f\"Top 5 binary edges in {lid}:\\n\", top)\n",
    "        else:\n",
    "            # Try pandas-like\n",
    "            try:\n",
    "                df = EV\n",
    "                if hasattr(df, \"query\"):\n",
    "                    bf = (\n",
    "                        df.query(\"kind == 'binary'\")\n",
    "                        .sort_values(\"effective_weight\", ascending=False)\n",
    "                        .head(5)\n",
    "                    )\n",
    "                    print(bf[[\"edge_id\", \"source\", \"target\", \"effective_weight\"]])\n",
    "            except Exception:\n",
    "                pass\n",
    "except Exception as e:\n",
    "    print(\"edges_view failed softly:\", e)\n",
    "\n",
    "try:\n",
    "    NV = G.vertices_view()\n",
    "    LV = G.slices_view()\n",
    "    print(\"vertices view cols:\", getattr(NV, \"columns\", None))\n",
    "    print(\"slices view cols:\", getattr(LV, \"columns\", None))\n",
    "except Exception as e:\n",
    "    print(\"vertices_view/slices_view failed softly:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57cd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presence queries\n",
    "any_e = next(iter(G.edge_to_idx.keys()))\n",
    "print(\"Edge presence across slices:\", G.edge_presence_across_slices(edge_id=any_e))\n",
    "\n",
    "any_p = random.choice(proteins)\n",
    "print(\"vertex presence across slices:\", G.vertex_presence_across_slices(any_p))\n",
    "\n",
    "# Hyperedge presence by members and head/tail\n",
    "if complex_ids:\n",
    "    m = random.choice(complex_ids)\n",
    "    members = G.hyperedge_definitions[m].get(\"members\", set())\n",
    "    if members:\n",
    "        print(\n",
    "            \"Hyperedge presence (members):\",\n",
    "            G.hyperedge_presence_across_slices(members=set(members)),\n",
    "        )\n",
    "if cascade_ids:\n",
    "    h = random.choice(cascade_ids)\n",
    "    hd = G.hyperedge_definitions[h]\n",
    "    if hd.get(\"head\") and hd.get(\"tail\"):\n",
    "        print(\n",
    "            \"Hyperedge presence (head/tail):\",\n",
    "            G.hyperedge_presence_across_slices(head=set(hd[\"head\"]), tail=set(hd[\"tail\"])),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traversal\n",
    "q = random.choice(proteins)\n",
    "print(f\"Neighbors({q}) sample:\", G.neighbors(q)[:10])\n",
    "print(f\"Out({q}) sample:\", G.out_neighbors(q)[:10])\n",
    "print(f\"In({q}) sample:\", G.in_neighbors(q)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721928c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice analytics, conserved/specific, temporal\n",
    "stats = G.slice_statistics()\n",
    "print(\"slice stats keys:\", list(stats.keys())[:5])\n",
    "\n",
    "conserved = G.conserved_edges(min_slices=len(sliceS))\n",
    "print(\"Conserved edges (present in all slices):\", len(conserved))\n",
    "\n",
    "disease_specific = G.slice_specific_edges(\"Disease\")\n",
    "print(\"Disease-specific edges:\", len(disease_specific))\n",
    "\n",
    "changes_e = G.temporal_dynamics(ORDERED_FOR_TEMPORAL, metric=\"edge_change\")\n",
    "changes_n = G.temporal_dynamics(ORDERED_FOR_TEMPORAL, metric=\"vertex_change\")\n",
    "print(\"Temporal edge changes entries:\", len(changes_e), \"vertex changes entries:\", len(changes_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice set ops and derived slices\n",
    "u = G.slice_union([\"Healthy\", \"Stressed\"])\n",
    "i = G.slice_intersection([\"Healthy\", \"Stressed\"])\n",
    "d = G.slice_difference(\"Healthy\", \"Stressed\")\n",
    "print(\"Union edges>=intersection edges:\", len(u[\"edges\"]) >= len(i[\"edges\"]))\n",
    "\n",
    "lid_u = G.create_slice_from_operation(\"HS_union\", u, desc=\"H∪S\")\n",
    "lid_i = G.create_slice_from_operation(\"HS_intersection\", i, desc=\"H∩S\")\n",
    "lid_d = G.create_slice_from_operation(\"H_minus_S\", d, desc=\"H\\\\S\")\n",
    "print(\n",
    "    \"Derived slices exist:\",\n",
    "    G.has_slice(\"HS_union\"),\n",
    "    G.has_slice(\"HS_intersection\"),\n",
    "    G.has_slice(\"H_minus_S\"),\n",
    ")\n",
    "\n",
    "# Aggregations\n",
    "G.create_aggregated_slice([\"Healthy\", \"Stressed\"], \"Agg_union\", method=\"union\", tag=\"agg_u\")\n",
    "G.create_aggregated_slice(\n",
    "    [\"Healthy\", \"Stressed\"], \"Agg_intersection\", method=\"intersection\", tag=\"agg_i\"\n",
    ")\n",
    "print(\"Aggregated slices added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge list & global counts\n",
    "el = G.edge_list()\n",
    "print(\"Edge list tuple length check (u,v,kind,id):\", all(len(t) == 4 for t in el))\n",
    "print(\"Global entity/edge counts:\", G.global_entity_count(), G.global_edge_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgraph & copy\n",
    "SG = G.subgraph_from_slice(\"DrugB\", resolve_slice_weights=True)\n",
    "print(\"DrugB subgraph vertices/edges:\", SG.number_of_vertices(), SG.number_of_edges())\n",
    "\n",
    "CP = G.copy()\n",
    "# quick consistency checks\n",
    "assert set(CP.vertices()) == set(G.vertices())\n",
    "assert set(CP.edges()) == set(G.edges())\n",
    "any_hyper = next(e for e, k in G.edge_kind.items() if k == \"hyper\")\n",
    "assert CP.edge_kind.get(any_hyper) == \"hyper\"\n",
    "for lid in G.list_slices(include_default=True):\n",
    "    assert CP._slices[lid][\"vertices\"] == G._slices[lid][\"vertices\"]\n",
    "    assert CP._slices[lid][\"edges\"] == G._slices[lid][\"edges\"]\n",
    "print(\"Deep copy OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removals: drop a slice of vertices/edges\n",
    "\n",
    "# Drop ~1% of proteins\n",
    "drop_vertices = random.sample(proteins, max(1, len(proteins) // 100))\n",
    "G.remove_vertices(drop_vertices)  # one pass\n",
    "\n",
    "# Drop up to 500 edges\n",
    "drop_edges = list(G.edge_to_idx.keys())[: min(500, len(G.edge_to_idx))]\n",
    "G.remove_edges(drop_edges)  # one pass\n",
    "\n",
    "print(\"After removals: vertices=\", G.number_of_vertices(), \"edges=\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audit & memory\n",
    "audit = G.audit_attributes()\n",
    "mem_bytes = G.memory_usage()\n",
    "print(\"Audit keys:\", list(audit.keys())[:10])\n",
    "print(\"Approx memory usage (bytes):\", int(mem_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing summary\n",
    "import pandas as pd\n",
    "\n",
    "timings = {\n",
    "    \"build_slices\": build_slices_time,\n",
    "    \"build_vertices\": build_vertices_time,\n",
    "    \"build_binary_edges\": build_binary_time,\n",
    "    \"build_hyperedges\": build_hyper_time,\n",
    "    \"build_vertexedge\": build_vertexedge_time,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sorted(timings.items(), key=lambda x: x[0]), columns=[\"stage\", \"seconds\"])\n",
    "\n",
    "print(f\"N_PROTEINS = 200_000\\nN_TRANSCRIPTS = 50_000\\nN_METABOLITES = 20_000\\nN_EDGE_ENTITIES = 20_500\\nN_BIN_EDGES = 800_000\\nN_HYPER_COMPLEX = 20, 000\\nN_HYPER_CASCADE = 20_000N_vertex_EDGE_BIDIR = 15_000\")\n",
    "\n",
    "print(\"Build timings (seconds)\", df)\n",
    "\n",
    "# Simple chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(df[\"stage\"], df[\"seconds\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"seconds\")\n",
    "plt.title(\"Graph Build Timings\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9caf79-3ebf-4afb-8fe9-3dd76c2409c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('stress'); out_dir.mkdir(exist_ok=True, parents=True)\n",
    "demo_path = out_dir/'demo.annnet'\n",
    "\n",
    "G.write(demo_path, overwrite=True)  # lossless save\n",
    "print('Wrote:', demo_path)\n",
    "\n",
    "import annnet\n",
    "# Round-trip check\n",
    "G2 = annnet.Graph.read(demo_path)\n",
    "print('Round-trip OK?', (G2.num_vertices, G2.num_edges) == (G.num_vertices, G.num_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ea4e2-60f7-4a72-aa59-e315767be7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
